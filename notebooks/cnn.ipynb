{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "This is a convolutional neural network that measures population density from birds-eye view images. It takes-in an input image and then detects the population density into 4 outputs:\n",
    "- 0 people = Zero Density\n",
    "- 1 to 5 people = Low Density\n",
    "- 6 to 30 = Medium Density\n",
    "- 31 to infinity = High Density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Constants\n",
    "Let's define some constants to use in our neural network for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy\n",
    "import numpy as np\n",
    "\n",
    "# set the random number generator of numpy\n",
    "from numpy.random import seed\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of training and testing images\n",
    "num_training_samples = 100\n",
    "num_testing_samples = 60\n",
    "\n",
    "# number of images to process before the weights are updated\n",
    "training_batch_size = 10\n",
    "testing_batch_size = 6\n",
    "\n",
    "# image's square dimension for the neural network (width x height)\n",
    "image_size = 224\n",
    "\n",
    "# steps is the number of images per epoch\n",
    "training_steps = np.ceil(num_training_samples / training_batch_size)\n",
    "testing_steps = np.ceil(num_testing_samples / testing_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing Image Data\n",
    "Let's pre-process image data using Image Augmentation from Keras's ImageDataGenerator class.\n",
    "\n",
    "Image augmentation allows us to create many batches of the images, which create many more diverse set of the images. Some augmentations could be rotating, stretching, zooming, etc.\n",
    "\n",
    "This helps prevent overfitting because augmentation better diversifies the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# seed the random number generator of tensorflow\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the keras image data augmentor\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# create a data generator for the training set using the built-in mobilenet functions\n",
    "training_batches = ImageDataGenerator(\n",
    "    preprocessing_function = keras.applications.mobilenet.preprocess_input\n",
    ").flow_from_directory(\n",
    "    \"images/training\",\n",
    "    target_size = (image_size, image_size),\n",
    "    batch_size = training_batch_size,\n",
    "    color_mode = \"grayscale\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data generator for the testing set using the built-in mobilenet functions\n",
    "testing_batches = ImageDataGenerator(\n",
    "    preprocessing_function = keras.applications.mobilenet.preprocess_input\n",
    ").flow_from_directory(\n",
    "    \"images/testing\",\n",
    "    target_size = (image_size, image_size),\n",
    "    batch_size = testing_batch_size,\n",
    "    color_mode = \"grayscale\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "Let's create a convolutional neural network to classify the image based on the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras packages to make the CNN to classify 2D images (width and height)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a sequential neural network model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1102 16:07:50.943714 139675414251328 deprecation.py:506] From /home/pravat/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# add the convolutional layer\n",
    "model.add(\n",
    "    Convolution2D(\n",
    "        filters = 3, # detect 3 features from the images because the humans are small blobs\n",
    "        kernel_size = (3, 3), # each feature detector matrix is 3x3\n",
    "        input_shape = (image_size, image_size, 1), # the images are black and white (1D)\n",
    "        activation = \"relu\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# add the max pooling layer as a 2x2 matrix\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# dropout some of the max pooled layer, then add the flattening layer\n",
    "model.add(Dropout(rate = 0.20))\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the hidden layers\n",
    "model.add(Dense(units = 100, activation = \"softmax\"))\n",
    "model.add(Dropout(rate = 0.10))\n",
    "\n",
    "# add the output layer\n",
    "model.add(Dense(units = 4, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics of the Model\n",
    "Let's create metrics (such as accuracy) to judge the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the callbacks for the models to use to fit to the training set\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model using the adam (stochastic aGradient Descent) optimizer\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\"accuracy\", \"categorical_accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoints\n",
    "Let's define checkpoints to save the neural network whenever we reached the most accurate version of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare a checkpoint to save the best version of the model\n",
    "filepath = \"model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_top_3_accuracy', verbose=1,\n",
    "                             save_best_only=True, mode='max')\n",
    "\n",
    "# reduce the learning rate as the learning stagnates\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_top_3_accuracy', factor=0.5, patience=2,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "\n",
    "callbacks_list = [checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting The Model\n",
    "Let's fit the model to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3793 - acc: 0.2333 - categorical_accuracy: 0.2333"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1102 16:07:53.382339 139675414251328 callbacks.py:989] Can save best model only with val_top_3_accuracy available, skipping.\n",
      "W1102 16:07:53.389621 139675414251328 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_top_3_accuracy` which is not available. Available metrics are: loss,acc,categorical_accuracy,val_loss,val_acc,val_categorical_accuracy,lr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 2s 166ms/step - loss: 1.3834 - acc: 0.2200 - categorical_accuracy: 0.2200 - val_loss: 1.3752 - val_acc: 0.2500 - val_categorical_accuracy: 0.2500\n",
      "Epoch 2/10\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3704 - acc: 0.2778 - categorical_accuracy: 0.2778"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1102 16:07:54.555004 139675414251328 callbacks.py:989] Can save best model only with val_top_3_accuracy available, skipping.\n",
      "W1102 16:07:54.556722 139675414251328 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_top_3_accuracy` which is not available. Available metrics are: loss,acc,categorical_accuracy,val_loss,val_acc,val_categorical_accuracy,lr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 116ms/step - loss: 1.3723 - acc: 0.2600 - categorical_accuracy: 0.2600 - val_loss: 1.3561 - val_acc: 0.3200 - val_categorical_accuracy: 0.3200\n",
      "Epoch 3/10\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3478 - acc: 0.2889 - categorical_accuracy: 0.2889"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1102 16:07:55.722670 139675414251328 callbacks.py:989] Can save best model only with val_top_3_accuracy available, skipping.\n",
      "W1102 16:07:55.723908 139675414251328 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_top_3_accuracy` which is not available. Available metrics are: loss,acc,categorical_accuracy,val_loss,val_acc,val_categorical_accuracy,lr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 116ms/step - loss: 1.3494 - acc: 0.2900 - categorical_accuracy: 0.2900 - val_loss: 1.3526 - val_acc: 0.3300 - val_categorical_accuracy: 0.3300\n",
      "Epoch 4/10\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3568 - acc: 0.3222 - categorical_accuracy: 0.3222"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1102 16:07:56.789936 139675414251328 callbacks.py:989] Can save best model only with val_top_3_accuracy available, skipping.\n",
      "W1102 16:07:56.791731 139675414251328 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_top_3_accuracy` which is not available. Available metrics are: loss,acc,categorical_accuracy,val_loss,val_acc,val_categorical_accuracy,lr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 107ms/step - loss: 1.3575 - acc: 0.3100 - categorical_accuracy: 0.3100 - val_loss: 1.3445 - val_acc: 0.3200 - val_categorical_accuracy: 0.3200\n",
      "Epoch 5/10\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3544 - acc: 0.3000 - categorical_accuracy: 0.3000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1102 16:07:57.932302 139675414251328 callbacks.py:989] Can save best model only with val_top_3_accuracy available, skipping.\n",
      "W1102 16:07:57.933783 139675414251328 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_top_3_accuracy` which is not available. Available metrics are: loss,acc,categorical_accuracy,val_loss,val_acc,val_categorical_accuracy,lr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 114ms/step - loss: 1.3537 - acc: 0.2900 - categorical_accuracy: 0.2900 - val_loss: 1.3453 - val_acc: 0.3100 - val_categorical_accuracy: 0.3100\n",
      "Epoch 6/10\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3421 - acc: 0.3444 - categorical_accuracy: 0.3444"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1102 16:07:59.186231 139675414251328 callbacks.py:989] Can save best model only with val_top_3_accuracy available, skipping.\n",
      "W1102 16:07:59.189051 139675414251328 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_top_3_accuracy` which is not available. Available metrics are: loss,acc,categorical_accuracy,val_loss,val_acc,val_categorical_accuracy,lr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 125ms/step - loss: 1.3436 - acc: 0.3300 - categorical_accuracy: 0.3300 - val_loss: 1.3368 - val_acc: 0.3700 - val_categorical_accuracy: 0.3700\n",
      "Epoch 7/10\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3278 - acc: 0.4000 - categorical_accuracy: 0.4000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1102 16:08:00.441146 139675414251328 callbacks.py:989] Can save best model only with val_top_3_accuracy available, skipping.\n",
      "W1102 16:08:00.442847 139675414251328 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_top_3_accuracy` which is not available. Available metrics are: loss,acc,categorical_accuracy,val_loss,val_acc,val_categorical_accuracy,lr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 125ms/step - loss: 1.3274 - acc: 0.3900 - categorical_accuracy: 0.3900 - val_loss: 1.3350 - val_acc: 0.3400 - val_categorical_accuracy: 0.3400\n",
      "Epoch 8/10\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3290 - acc: 0.3556 - categorical_accuracy: 0.3556"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1102 16:08:01.613293 139675414251328 callbacks.py:989] Can save best model only with val_top_3_accuracy available, skipping.\n",
      "W1102 16:08:01.614306 139675414251328 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_top_3_accuracy` which is not available. Available metrics are: loss,acc,categorical_accuracy,val_loss,val_acc,val_categorical_accuracy,lr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 117ms/step - loss: 1.3361 - acc: 0.3300 - categorical_accuracy: 0.3300 - val_loss: 1.3341 - val_acc: 0.3400 - val_categorical_accuracy: 0.3400\n",
      "Epoch 9/10\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3338 - acc: 0.3333 - categorical_accuracy: 0.3333"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1102 16:08:02.807259 139675414251328 callbacks.py:989] Can save best model only with val_top_3_accuracy available, skipping.\n",
      "W1102 16:08:02.809024 139675414251328 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_top_3_accuracy` which is not available. Available metrics are: loss,acc,categorical_accuracy,val_loss,val_acc,val_categorical_accuracy,lr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 119ms/step - loss: 1.3310 - acc: 0.3400 - categorical_accuracy: 0.3400 - val_loss: 1.3254 - val_acc: 0.3800 - val_categorical_accuracy: 0.3800\n",
      "Epoch 10/10\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3272 - acc: 0.3778 - categorical_accuracy: 0.3778"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1102 16:08:04.005440 139675414251328 callbacks.py:989] Can save best model only with val_top_3_accuracy available, skipping.\n",
      "W1102 16:08:04.006915 139675414251328 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_top_3_accuracy` which is not available. Available metrics are: loss,acc,categorical_accuracy,val_loss,val_acc,val_categorical_accuracy,lr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 120ms/step - loss: 1.3282 - acc: 0.3700 - categorical_accuracy: 0.3700 - val_loss: 1.3346 - val_acc: 0.3700 - val_categorical_accuracy: 0.3700\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    training_batches,\n",
    "    steps_per_epoch = training_steps,\n",
    "    validation_data = testing_batches,\n",
    "    validation_steps = testing_steps,\n",
    "    epochs = 10,\n",
    "    verbose = 1,\n",
    "    callbacks = callbacks_list\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
